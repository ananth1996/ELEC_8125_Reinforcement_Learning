{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c76ebde64137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# from rbf_agent import Agent as RBFAgent  # Use for Tasks 1-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# from dqn_agent import Agent as DQNAgent  # Task 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mExercises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEx4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrbf_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mRBFAgent\u001b[0m  \u001b[1;31m# Use for Tasks 1-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mExercises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEx4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mDQNAgent\u001b[0m  \u001b[1;31m# Task 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Aalto Course Docs\\ELEC-E8125 - Reinforcement learning\\Exercises\\Ex4\\rbf_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# from Exercises.Ex4.utils import ReplayMemory, Transition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_approximation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRBFSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "try:\n",
    "    notebook = True\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")\n",
    "    from Exercises.Ex4.rbf_agent import Agent as RBFAgent  # Use for Tasks 1-3\n",
    "    from Exercises.Ex4.dqn_agent import Agent as DQNAgent  # Task 4\n",
    "    from Exercises.Ex4.utils import plot_rewards\n",
    "\n",
    "except:\n",
    "    notebook = False\n",
    "    from utils import plot_rewards\n",
    "    from rbf_agent import Agent as RBFAgent  # Use for Tasks 1-3\n",
    "    from dqn_agent import Agent as DQNAgent  # Task 4\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# from rbf_agent import Agent as RBFAgent  # Use for Tasks 1-3\n",
    "# from dqn_agent import Agent as DQNAgent  # Task 4\n",
    "\n",
    "from itertools import count\n",
    "import torch\n",
    "# from utils import plot_rewards\n",
    "\n",
    "env_name = \"CartPole-v0\"\n",
    "#env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "# Set hyperparameters\n",
    "# Values for RBF (Tasks 1-3)\n",
    "glie_a = 50\n",
    "num_episodes = 1000\n",
    "\n",
    "# Values for DQN  (Task 4)\n",
    "# if \"CartPole\" in env_name:\n",
    "#     TARGET_UPDATE = 20\n",
    "#     glie_a = 200\n",
    "#     num_episodes = 5000\n",
    "#     hidden = 12\n",
    "#     gamma = 0.98\n",
    "#     replay_buffer_size = 50000\n",
    "#     batch_size = 32\n",
    "# elif \"LunarLander\" in env_name:\n",
    "#     TARGET_UPDATE = 20\n",
    "#     glie_a = 5000\n",
    "#     num_episodes = 15000\n",
    "#     hidden = 64\n",
    "#     gamma = 0.95\n",
    "#     replay_buffer_size = 50000\n",
    "#     batch_size = 128\n",
    "# else:\n",
    "#     raise ValueError(\"Please provide hyperparameters for %s\" % env_name)\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Tasks 1-3 - RBF\n",
    "agent = RBFAgent(n_actions)\n",
    "\n",
    "# Task 4 - DQN\n",
    "# agent = DQNAgent(state_space_dim, n_actions, replay_buffer_size, batch_size,\n",
    "#               hidden, gamma)\n",
    "\n",
    "# Training loop\n",
    "#%%\n",
    "\n",
    "\n",
    "\n",
    "# Task 3 - plot the policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('d:/Aalto Course Docs/ELEC-E8125 - Reinforcement learning')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}